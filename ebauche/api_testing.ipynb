{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbe3b3c-0e06-4631-915d-fe812a713de9",
   "metadata": {},
   "source": [
    "# Analyse des schémas de comportement des utilisateurs dans les applications DeFI\n",
    "## Analyse des API Web 3 en vue de constituer un jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9d5e8",
   "metadata": {},
   "source": [
    "### API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30c1f8c0-b71d-4ca8-8663-42e1d0572a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "cg_api_key = os.getenv('CG_API_KEY')\n",
    "eth_api_key = os.getenv('ETH_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa95cdb-3aae-4f42-9b9b-0282a82a83fd",
   "metadata": {},
   "source": [
    "### PROTOCOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e1cdd-a524-4b55-b416-ef5a5e15bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction des données pour les protocoles clés...\n",
      "11 protocoles récupérés.\n",
      "Données sauvegardées dans 'key_protocols.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "KEY_PROTOCOLS = [                                   # Liste des protocoles clés et leurs identifiants sur CoinGecko\n",
    "    \"uniswap\",          # DEX\n",
    "    \"curve-dao-token\",  # DEX \n",
    "    \"balancer\",         # DEX\n",
    "    \"aave\",             # Lending\n",
    "    \"maker\",            # Lending\n",
    "    \"yearn-finance\",    # Yield Farming\n",
    "    \"harvest-finance\",  # Yield Farming\n",
    "    \"dai\",              # Stablecoin\n",
    "    \"usd-coin\",         # Stablecoin\n",
    "    \"tether\",           # Stablecoin\n",
    "    \"nftfi\",            # NFT-Fi (optionnel)\n",
    "]\n",
    "\n",
    "def get_defi_protocol_info(protocol_id):\n",
    "    \"\"\"\n",
    "    Récupère les informations détaillées d'un protocole spécifique.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.coingecko.com/api/v3/coins/{protocol_id}\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-cg-demo-api-key\": cg_api_key                 # Clé API CoinGecko\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            platforms = data.get('platforms', {})\n",
    "\n",
    "            protocol_info = {\n",
    "                'name': data.get('name', 'N/A'),\n",
    "                'type': infer_protocol_type(protocol_id),           # Déduction du type selon la liste fournie\n",
    "                'blockchain_contracts': [\n",
    "                    {'blockchain': blockchain, 'contract': contract}\n",
    "                    for blockchain, contract in platforms.items()\n",
    "                ],\n",
    "                'website_url': data.get('links', {}).get('homepage', ['N/A'])[0],           # URL officielle\n",
    "                'symbol': data.get('symbol', 'N/A'),                                        # Symbole du token associé\n",
    "                'market_cap_rank': data.get('market_cap_rank', 'N/A'),                      # Classement par capitalisation\n",
    "                'description': data.get('description', {}).get('en', '')[:250],             # Description limitée à 250 caractères\n",
    "            }\n",
    "            return protocol_info\n",
    "        \n",
    "        elif response.status_code == 404:\n",
    "            print(f\"Protocole introuvable : {protocol_id}\")\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Trop de requêtes. Attente de 10 secondes...\")\n",
    "            time.sleep(10)                                              # Attendre 10 secondes en cas de limite d'API\n",
    "            return get_defi_protocol_info(protocol_id)                  # Réessayer\n",
    "        else:\n",
    "            print(f\"Erreur inconnue avec {protocol_id}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {protocol_id} : {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def infer_protocol_type(protocol_id):\n",
    "    \"\"\"\n",
    "    Déduit le type du protocole en fonction de son identifiant.\n",
    "    \"\"\"\n",
    "    if protocol_id in [\"uniswap\", \"sushiswap\", \"curve-dao-token\", \"balancer\"]:\n",
    "        return \"DEX\"\n",
    "    elif protocol_id in [\"aave\", \"compound\", \"maker\"]:\n",
    "        return \"Lending\"\n",
    "    elif protocol_id in [\"yearn-finance\", \"harvest-finance\", \"curve-dao-token\"]:\n",
    "        return \"Yield Farming\"\n",
    "    elif protocol_id in [\"dai\", \"usd-coin\", \"tether\"]:\n",
    "        return \"Stablecoin\"\n",
    "    elif protocol_id in [\"nftfi\", \"nifty-gateway\", \"opensea\"]:\n",
    "        return \"NFT-Fi\"\n",
    "    else:\n",
    "        return \"DeFi\"\n",
    "\n",
    "def save_protocols_to_json(protocols, filename):\n",
    "    \"\"\"\n",
    "    Sauvegarde une liste de protocoles dans un fichier JSON.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(protocols, f, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":                                                  # Exécuter le script                  \n",
    "    print(\"Extraction des données pour les protocoles clés...\")\n",
    "    protocols_data = []\n",
    "    for protocol_id in KEY_PROTOCOLS:\n",
    "        protocol_info = get_defi_protocol_info(protocol_id)\n",
    "        if protocol_info:\n",
    "            protocols_data.append(protocol_info)\n",
    "                                                                            # Respecter la limite de requêtes : attendre 2 secondes entre chaque requête\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(f\"{len(protocols_data)} protocoles récupérés.\")\n",
    "    \n",
    "    save_protocols_to_json(protocols_data, 'key_protocols.json')        # Sauvegarder dans un fichier JSON\n",
    "    print(\"Données sauvegardées dans 'key_protocols.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870e873",
   "metadata": {},
   "source": [
    "### CONTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eafa6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ont été enregistrées dans 'contracts_data.csv'.\n",
      "Nombre de contrats associés à chaque blockchain :\n",
      "- Ethereum: 11 contrats\n",
      "- Avalanche: 7 contrats\n",
      "- Energi: 7 contrats\n",
      "- Polygon-pos: 7 contrats\n",
      "- Near-protocol: 6 contrats\n",
      "- Optimistic-ethereum: 6 contrats\n",
      "- Arbitrum-one: 6 contrats\n",
      "- Sora: 5 contrats\n",
      "- Harmony-shard-0: 4 contrats\n",
      "- Huobi-token: 4 contrats\n",
      "- Binance-smart-chain: 3 contrats\n",
      "- Fantom: 3 contrats\n",
      "- Xdai: 3 contrats\n",
      "- Base: 3 contrats\n",
      "- Celo: 2 contrats\n",
      "- Tron: 2 contrats\n",
      "- Solana: 2 contrats\n",
      "- Aptos: 2 contrats\n",
      "- Algorand: 1 contrats\n",
      "- Kava: 1 contrats\n",
      "- Stellar: 1 contrats\n",
      "- Polygon-zkevm: 1 contrats\n",
      "- Sui: 1 contrats\n",
      "- Hedera-hashgraph: 1 contrats\n",
      "- Polkadot: 1 contrats\n",
      "- Zksync: 1 contrats\n",
      "- The-open-network: 1 contrats\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('key_protocols.json', 'r') as f:\n",
    "    protocols_data = json.load(f)\n",
    "\n",
    "data = []\n",
    "\n",
    "for protocol in protocols_data:                                         # Parcours des protocoles pour extraire les contrats          \n",
    "    blockchain_contracts = protocol.get(\"blockchain_contracts\", [])\n",
    "    \n",
    "    for contract_info in blockchain_contracts:                          # Parcours des contrats de chaque protocole\n",
    "        contract_address = contract_info.get(\"contract\")\n",
    "        blockchain = contract_info.get(\"blockchain\").lower()  \n",
    "        \n",
    "        data.append({\n",
    "            \"protocol\": protocol.get(\"name\"),\n",
    "            \"blockchain\": blockchain,\n",
    "            \"contract\": contract_address\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('contracts_data.csv', index=False)\n",
    "\n",
    "print(\"Les données ont été enregistrées dans 'contracts_data.csv'.\")\n",
    "\n",
    "blockchain_counts = df['blockchain'].value_counts()\n",
    "\n",
    "print(\"Nombre de contrats associés à chaque blockchain :\")\n",
    "for blockchain, count in blockchain_counts.items():\n",
    "    print(f\"- {blockchain.capitalize()}: {count} contrats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac2e15f1-cae0-45c7-b39f-575f2c3882d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV chargé avec succès.\n",
      "Nombre de contrats Ethereum trouvés: 11\n",
      "Calcul des blocs pour la période 2024-12-01 - 2024-12-31.\n",
      "Start timestamp: 1733007600, End timestamp: 1735599600\n",
      "Blocs trouvés: start_block = 21303634, end_block = 21518436\n",
      "Récupération des logs pour le contrat 0x1f9840a85d5af5bf1d1762f925bdaddc4201f984 entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0x1f9840a85d5af5bf1d1762f925bdaddc4201f984, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0xd533a949740bb3306d119cc777fa900ba034cd52 entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0xd533a949740bb3306d119cc777fa900ba034cd52, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0xba100000625a3754423978a60c9317c58a424e3d entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0xba100000625a3754423978a60c9317c58a424e3d, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0x7fc66500c84a76ad7e9c93437bfc5ac33e2ddae9 entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0x7fc66500c84a76ad7e9c93437bfc5ac33e2ddae9, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2 entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0x0bc529c00c6401aef6d220be8c6ea1667f6ad93e entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0x0bc529c00c6401aef6d220be8c6ea1667f6ad93e, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0xa0246c9032bc3a600820415ae600c6388619a14d entre les blocs 21303634 et 21518436.\n",
      "Logs récupérés pour le contrat 0xa0246c9032bc3a600820415ae600c6388619a14d, nombre de logs: 4254.\n",
      "Récupération des logs pour le contrat 0x6b175474e89094c44da98b954eedeac495271d0f entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0x6b175474e89094c44da98b954eedeac495271d0f, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48 entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0xdac17f958d2ee523a2206206994597c13d831ec7 entre les blocs 21303634 et 21518436.\n",
      "Erreur lors de la récupération des logs: {'status': '0', 'message': 'Result window is too large, PageNo x Offset size must be less than or equal to 10000', 'result': None}\n",
      "Logs récupérés pour le contrat 0xdac17f958d2ee523a2206206994597c13d831ec7, nombre de logs: 10000.\n",
      "Récupération des logs pour le contrat 0x09d6f0f5a21f5be4f59e209747e2d07f50bc694c entre les blocs 21303634 et 21518436.\n",
      "Logs récupérés pour le contrat 0x09d6f0f5a21f5be4f59e209747e2d07f50bc694c, nombre de logs: 492.\n",
      "Nombre de lignes dans le DataFrame avant suppression des doublons: 94746\n",
      "Nombre de lignes après suppression des doublons: 11\n",
      "Les données des utilisateurs ont été enregistrées dans 'user_contracts_december_2024.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "# Chargement du CSV avec les contrats Ethereum\n",
    "try:\n",
    "    df = pd.read_csv('contracts_data.csv')\n",
    "    print(\"Fichier CSV chargé avec succès.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du fichier CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Filtrage des contrats Ethereum\n",
    "ethereum_contracts = df[df['blockchain'] == 'ethereum']\n",
    "print(f\"Nombre de contrats Ethereum trouvés: {len(ethereum_contracts)}\")\n",
    "\n",
    "# Liste pour stocker les données des utilisateurs\n",
    "user_data = []\n",
    "block_data = {}  # Dictionnaire pour stocker les blocs par contrat\n",
    "\n",
    "# Set pour éviter les doublons de logs\n",
    "seen_logs = set()\n",
    "\n",
    "# Fonction pour récupérer les logs d'un contrat avec pagination\n",
    "def get_contract_logs(contract_address, start_block, end_block):\n",
    "    print(f\"Récupération des logs pour le contrat {contract_address} entre les blocs {start_block} et {end_block}.\")\n",
    "    url = f'https://api.etherscan.io/api'\n",
    "    params = {\n",
    "        'module': 'logs',\n",
    "        'action': 'getLogs',\n",
    "        'fromBlock': start_block,\n",
    "        'toBlock': end_block,\n",
    "        'address': contract_address,\n",
    "        'apikey': eth_api_key,\n",
    "        'page': 1,  # On commence par la première page\n",
    "        'offset': 1000  # Limité à 1000 logs par page\n",
    "    }\n",
    "    \n",
    "    all_logs = []  # Liste pour stocker tous les logs récupérés\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()  # Vérifie si la réponse est réussie (status code 200)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('status') == '1':  # Si l'appel API a réussi\n",
    "                logs = data.get('result', [])\n",
    "                all_logs.extend(logs)  # Ajouter les logs de cette page à la liste\n",
    "                \n",
    "                if len(logs) < 1000:  # Si moins de 1000 logs, on a récupéré tous les logs\n",
    "                    break\n",
    "                else:\n",
    "                    params['page'] += 1  # Passer à la page suivante\n",
    "                    time.sleep(0.2)  # Respecter la limite de 5 appels par seconde (1 appel toutes les 200ms)\n",
    "            else:\n",
    "                print(f\"Erreur lors de la récupération des logs: {data}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur lors de l'appel API pour récupérer les logs du contrat {contract_address}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_logs\n",
    "\n",
    "# Fonction pour convertir un timestamp Unix en date formatée\n",
    "def convert_timestamp_to_date(timestamp):\n",
    "    try:\n",
    "        timestamp = int(timestamp, 16)  # Conversion d'un timestamp hexadécimal en entier\n",
    "        return datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')  # Utilisation de timezone.utc\n",
    "    except ValueError as e:\n",
    "        print(f\"Erreur lors de la conversion du timestamp: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour récupérer les blocs correspondant à une période\n",
    "def get_block_range_for_date_range(start_date, end_date):\n",
    "    print(f\"Calcul des blocs pour la période {start_date} - {end_date}.\")\n",
    "    # Convertir les dates en timestamps Unix\n",
    "    start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "    end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
    "    \n",
    "    # Affichage des timestamps pour débogage\n",
    "    print(f\"Start timestamp: {start_timestamp}, End timestamp: {end_timestamp}\")\n",
    "    \n",
    "    # Calculer les blocs en fonction des timestamps (Etherscan API)\n",
    "    start_block_url = f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={start_timestamp}&closest=before&apikey={eth_api_key}'\n",
    "    end_block_url = f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={end_timestamp}&closest=after&apikey={eth_api_key}'\n",
    "\n",
    "    try:\n",
    "        start_block_response = requests.get(start_block_url).json()\n",
    "        end_block_response = requests.get(end_block_url).json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération des blocs: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    if start_block_response.get('status') == '1' and end_block_response.get('status') == '1':\n",
    "        start_block = int(start_block_response['result'])\n",
    "        end_block = int(end_block_response['result'])\n",
    "        print(f\"Blocs trouvés: start_block = {start_block}, end_block = {end_block}\")\n",
    "        return start_block, end_block\n",
    "    else:\n",
    "        print(f\"Erreur lors de la récupération des blocs. Start response: {start_block_response}, End response: {end_block_response}\")\n",
    "        return None, None\n",
    "\n",
    "# Calcul des blocs pour la période de décembre 2024 une seule fois\n",
    "start_date = '2024-12-01'\n",
    "end_date = '2024-12-31'\n",
    "start_block, end_block = get_block_range_for_date_range(start_date, end_date)\n",
    "\n",
    "# Vérification que les blocs ont bien été récupérés\n",
    "if not start_block or not end_block:\n",
    "    print(\"Erreur : Impossible de récupérer les blocs pour la période spécifiée.\")\n",
    "    exit(1)\n",
    "\n",
    "# Sauvegarde des numéros de blocs dans le dictionnaire pour chaque contrat\n",
    "block_data['start_block'] = start_block\n",
    "block_data['end_block'] = end_block\n",
    "\n",
    "# Parcours des contrats Ethereum pour extraire les logs du mois de décembre 2024\n",
    "for _, row in ethereum_contracts.iterrows():\n",
    "    contract_address = row['contract']\n",
    "    protocol_name = row['protocol']  # Le nom du protocole est extrait ici\n",
    "    \n",
    "    # Récupérer les logs pour ce contrat avec pagination\n",
    "    logs = get_contract_logs(contract_address, start_block, end_block)\n",
    "\n",
    "    if logs:  # Vérifie si des logs ont été récupérés\n",
    "        print(f\"Logs récupérés pour le contrat {contract_address}, nombre de logs: {len(logs)}.\")\n",
    "        for log in logs:\n",
    "            timestamp = log.get('timeStamp', None)\n",
    "            if timestamp:\n",
    "                created_at = convert_timestamp_to_date(timestamp)\n",
    "                if created_at:\n",
    "                    user_data.append({\n",
    "                        'user_id': log['address'],  # Adresse de l'utilisateur\n",
    "                        'address': contract_address,  # Adresse du contrat\n",
    "                        'blockchain': 'ethereum',  # Blockchain associée\n",
    "                        'protocol': protocol_name,  # Nom du protocole associé\n",
    "                        'created_at': created_at  # Date formatée\n",
    "                    })\n",
    "    else:\n",
    "        print(f\"Aucun log trouvé pour le contrat {contract_address}.\")\n",
    "\n",
    "# Création d'un DataFrame avec les données récupérées\n",
    "user_df = pd.DataFrame(user_data)\n",
    "print(f\"Nombre de lignes dans le DataFrame avant suppression des doublons: {len(user_df)}\")\n",
    "\n",
    "# Suppression des doublons dans le DataFrame basé sur les colonnes 'user_id' et 'address'\n",
    "user_df = user_df.drop_duplicates(subset=['user_id', 'address'])\n",
    "\n",
    "print(f\"Nombre de lignes après suppression des doublons: {len(user_df)}\")\n",
    "\n",
    "# Sauvegarde des données des utilisateurs dans un fichier CSV\n",
    "try:\n",
    "    user_df.to_csv('user_contracts_december_2024.csv', index=False)\n",
    "    print(\"Les données des utilisateurs ont été enregistrées dans 'user_contracts_december_2024.csv'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la sauvegarde des données dans le fichier CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1fc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6744c8c-344c-4961-ae69-7829561d7302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
