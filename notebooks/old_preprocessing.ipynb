{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing DeFi Data on Ethereum Blockchain to Understand Behaviors\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp conversion for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '../data/dataset/market.parquet'\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "df = parquet_file.read().to_pandas()\n",
    "\n",
    "for column in df.columns:\n",
    "    if pd.api.types.is_timedelta64_dtype(df[column]):\n",
    "        df[column] = df[column].astype('str')\n",
    "    elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "        df[column] = df[column].astype('str')\n",
    "\n",
    "output_path = '../data/dataset/market_converted.parquet'\n",
    "df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "print(f'Le fichier a été converti avec succès et sauvegardé sous {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(list, base_path='../data/dataset'):\n",
    "    dataframes = {}\n",
    "    for file in list:\n",
    "        file_path = f\"{base_path}/{file}.parquet\"\n",
    "        df = pd.read_parquet(file_path)\n",
    "        dataframes[file] = df\n",
    "        print(f\"DataFrame '{file}' contains columns:\\n{df.columns.tolist()}\\n\")\n",
    "    return dataframes\n",
    "\n",
    "dataframes = load_data(['users', 'market'])\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Dataframe {key}:\\n=============================================\\n\")\n",
    "    print(f\"{df.head(5)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Clean column names by stripping leading/trailing spaces, lowercasing all characters, and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Extraction of protocol types/names and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "users = dataframes['users'].copy()\n",
    "market = dataframes['market'].copy()\n",
    "\n",
    "#---------------------------------------------- Protocol Types\n",
    "\n",
    "def parse_protocols(protocol_column):\n",
    "    \"\"\"\n",
    "    Parses a JSON string into a dictionary. If the string is not a valid JSON, returns an empty dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        protocols = json.loads(protocol_column)\n",
    "        return protocols if isinstance(protocols, dict) else {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def process_user_protocols(users):\n",
    "    \"\"\"\n",
    "    Processes user protocols by extracting specific protocol types and adding them as separate columns.\n",
    "    \"\"\"\n",
    "    protocol_columns = ['type_' + k for k in ['DEX', 'Lending', 'Stablecoin', \"Yield Farming\", \"NFT-Fi\"]]\n",
    "    users = users.assign(**{col: 0 for col in protocol_columns})\n",
    "\n",
    "    users['parsed_protocols'] = users['protocol_types'].apply(parse_protocols)\n",
    "\n",
    "    for protocol in ['DEX', 'Lending', 'Stablecoin', \"Yield Farming\", \"NFT-Fi\"]:\n",
    "        column_name = f'type_{protocol}'\n",
    "        users[column_name] = users['parsed_protocols'].apply(lambda x: x.get(protocol, 0))\n",
    "    \n",
    "    return users.drop(columns=['protocol_types', 'parsed_protocols'])\n",
    "\n",
    "users = process_user_protocols(users)\n",
    "\n",
    "#---------------------------------------------- Protocl Used\n",
    "\n",
    "def transform_protocols_column(df, column_name='protocols_used'):\n",
    "    \"\"\"\n",
    "    Transforms the 'protocols_used' column by converting string representations of dictionaries into actual dictionaries.\n",
    "    Then, it creates new columns for each protocol used, counting the occurrences of each protocol.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[column_name] = df[column_name].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    count_columns = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        protocols = row[column_name]\n",
    "        for protocol_name, protocol_data in protocols.items():\n",
    "            count_columns.append(f'{protocol_name}_count')\n",
    "            df.at[index, f'{protocol_name}_count'] = int(protocol_data.get('count', 0))\n",
    "    \n",
    "    return df.drop(columns=[column_name])\n",
    "\n",
    "users = transform_protocols_column(users)\n",
    "\n",
    "#---------------------------------------------- Cleaning\n",
    "\n",
    "users = clean_column_names(users)\n",
    "users.fillna(0, inplace=True)\n",
    "protocols_counts = ['curve_dao_count', 'aave_count', 'tether_count', 'uniswap_count', 'maker_count', 'yearn.finance_count', 'usdc_count', 'dai_count', 'balancer_count', 'harvest_finance_count']\n",
    "users[protocols_counts] = users[protocols_counts].astype(int)\n",
    "\n",
    "print(users.columns)\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Extract transactions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transactions(row):\n",
    "    transactions = json.loads(row['transactions'])  # Convertir la chaîne JSON en liste de dictionnaires\n",
    "    extracted_rows = []\n",
    "    \n",
    "    for txn in transactions:\n",
    "        extracted_row = {\n",
    "            'address': row['address'],\n",
    "            'first_seen': row['first_seen'],\n",
    "            'last_seen': row['last_seen'],\n",
    "            'received_count': row['received_count'],\n",
    "            'total_received_(eth)': row['total_received_(eth)'],\n",
    "            'sent_count': row['sent_count'],\n",
    "            'total_sent_(eth)': row['total_sent_(eth)'],\n",
    "            'transactions': row['transactions'],  \n",
    "            'type_dex': row['type_dex'],\n",
    "            'type_lending': row['type_lending'],\n",
    "            'type_stablecoin': row['type_stablecoin'],\n",
    "            'type_yield_farming': row['type_yield_farming'],\n",
    "            'type_nft-fi': row['type_nft-fi'],\n",
    "            'curve_dao_count': row['curve_dao_count'],\n",
    "            'aave_count': row['aave_count'],\n",
    "            'tether_count': row['tether_count'],\n",
    "            'uniswap_count': row['uniswap_count'],\n",
    "            'maker_count': row['maker_count'],\n",
    "            'yearn.finance_count': row['yearn.finance_count'],\n",
    "            'usdc_count': row['usdc_count'],\n",
    "            'dai_count': row['dai_count'],\n",
    "            'balancer_count': row['balancer_count'],\n",
    "            'harvest_finance_count': row['harvest_finance_count'],\n",
    "            'tx_timestamp': txn['timestamp'],\n",
    "            'tx_protocol': txn['protocol_name'],\n",
    "            'tx_value_(eth)': txn['value (ETH)'],\n",
    "            'tx_is_sender': txn['is_sender'],\n",
    "            'tx_gas_used': txn['gas_used']\n",
    "        }\n",
    "        extracted_rows.append(extracted_row)\n",
    "    return extracted_rows\n",
    "\n",
    "expanded_rows = users.apply(extract_transactions, axis=1)\n",
    "\n",
    "users = pd.DataFrame([item for sublist in expanded_rows for item in sublist])\n",
    "\n",
    "users.drop(columns=['transactions'], inplace=True)\n",
    "print(users.columns)\n",
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(market.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users['tx_protocol'].unique())\n",
    "print(market['protocol_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_users_with_market(df_users, df_market, batch_size=1000):\n",
    "    df_users['tx_timestamp'] = pd.to_datetime(df_users['tx_timestamp'])\n",
    "    df_market['timestamp'] = pd.to_datetime(df_market['timestamp'])\n",
    "\n",
    "    market_columns = [col for col in df_market.columns if col not in ['timestamp', 'protocol_name']]\n",
    "\n",
    "    merged_df = df_users.copy()\n",
    "    for col in market_columns:\n",
    "        merged_df[col] = str(np.nan)\n",
    "    \n",
    "    grouped_market = df_market.groupby('protocol_name')\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(df_users), batch_size), desc=\"Processing users\", unit=\"lot\"):\n",
    "        batch_end = min(batch_start + batch_size, len(df_users))\n",
    "        batch = df_users.iloc[batch_start:batch_end]\n",
    "        \n",
    "        for idx, user_row in batch.iterrows():\n",
    "            protocol = user_row['tx_protocol']\n",
    "            tx_timestamp = user_row['tx_timestamp']\n",
    "\n",
    "            if protocol in grouped_market.groups:\n",
    "                relevant_market_data = grouped_market.get_group(protocol).copy()\n",
    "\n",
    "                relevant_market_data['time_diff'] = abs(relevant_market_data['timestamp'] - tx_timestamp)\n",
    "                closest_market_row = relevant_market_data.loc[relevant_market_data['time_diff'].idxmin()]\n",
    "\n",
    "                for col in market_columns:\n",
    "                    merged_df.at[idx, col] = closest_market_row[col]\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "data = merge_users_with_market(users, market)\n",
    "\n",
    "data = clean_column_names(data)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "data.to_parquet('../data/data.parquet', engine='pyarrow')\n",
    "\n",
    "print(data.columns)\n",
    "data.head(5)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6534258,
     "sourceId": 10568671,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
